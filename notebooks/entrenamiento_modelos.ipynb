{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8088bbf8-8f70-4527-b697-66a4be846417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 20:58:04 WARN Utils: Your hostname, vbox resolves to a loopback address: 127.0.1.1; using 192.168.18.70 instead (on interface enp0s3)\n",
      "25/11/23 20:58:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 20:58:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/23 20:58:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession iniciada correctamente\n",
      "Versión: 3.5.1\n",
      "Master: yarn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Java\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/zulu-8'\n",
    "# Python del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/debian1/BigData_UPAO/bigdata_env/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/debian1/BigData_UPAO/bigdata_env/bin/python3'\n",
    "\n",
    "# Archivos de configuración de Hadoop/YARN\n",
    "os.environ['HADOOP_CONF_DIR'] = '/opt/hadoop-3.3.6/etc/hadoop'\n",
    "os.environ['YARN_CONF_DIR'] = '/opt/hadoop-3.3.6/etc/hadoop'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"BetGol-Model\")\n",
    "        .master(\"yarn\")\n",
    "        .config(\"spark.driver.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"150\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession iniciada correctamente\")\n",
    "print(\"Versión:\", spark.version)\n",
    "print(\"Master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509dd770-f328-4098-b859-074ee26ff046",
   "metadata": {},
   "source": [
    "# Dividir train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1960f06b-dbb8-4429-9355-2faf7dceb9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente desde PROCESSED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas totales: 29348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_ml_ready = spark.read.parquet(\"hdfs:///user/johan/data/processed/\")\n",
    "\n",
    "print(\"Dataset cargado correctamente desde PROCESSED.\")\n",
    "print(\"Filas totales:\", df_ml_ready.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6391eb-5faf-4f23-a376-470a265970ef",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e8a0f3-33ea-468e-9aa6-b32bae5051ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIVISIÓN TRAIN / TEST (POR FECHA) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de Corte: 2024-01-01\n",
      "Entrenamiento: 26164 partidos (89.2%)\n",
      "Prueba:        3184 partidos (10.8%)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- DIVISIÓN TRAIN / TEST (POR FECHA) ---\")\n",
    "\n",
    "FECHA_CORTE = \"2024-01-01\"\n",
    "\n",
    "train_data = df_ml_ready.filter(col(\"fecha\") < FECHA_CORTE)\n",
    "test_data  = df_ml_ready.filter(col(\"fecha\") >= FECHA_CORTE)\n",
    "\n",
    "n_train = train_data.count()\n",
    "n_test  = test_data.count()\n",
    "total   = n_train + n_test\n",
    "\n",
    "print(f\"Fecha de Corte: {FECHA_CORTE}\")\n",
    "print(f\"Entrenamiento: {n_train} partidos ({n_train/total:.1%})\")\n",
    "print(f\"Prueba:        {n_test} partidos ({n_test/total:.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ad880-a00b-4682-9e29-0d5eaf8cefd2",
   "metadata": {},
   "source": [
    "# Resultado del Partido (1X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907451f-60e8-4a3b-befe-01608ec538e0",
   "metadata": {},
   "source": [
    "* numTrees: Usamos árboles para que voten (mayor estabilidad).\n",
    "* maxDepth: Profundidad media para capturar patrones sin memorizar (overfitting).\n",
    "* seed=42: Semilla para que el resultado sea reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8ee07a-fdc5-4e90-8401-29be95c7bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ENTRENAMIENTO DEL MODELO (Random Forest) ---\n",
      "Iniciando entrenamiento con datos históricos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:50:18 WARN DAGScheduler: Broadcasting large task binary with size 1296.2 KiB\n",
      "25/11/23 21:50:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/11/23 21:50:22 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/11/23 21:50:25 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/11/23 21:50:28 WARN DAGScheduler: Broadcasting large task binary with size 1325.7 KiB\n",
      "25/11/23 21:50:29 WARN DAGScheduler: Broadcasting large task binary with size 10.0 MiB\n",
      "25/11/23 21:50:32 WARN DAGScheduler: Broadcasting large task binary with size 1834.6 KiB\n",
      "25/11/23 21:50:34 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "25/11/23 21:50:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/11/23 21:50:40 WARN DAGScheduler: Broadcasting large task binary with size 21.2 MiB\n",
      "25/11/23 21:50:43 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/11/23 21:50:47 WARN DAGScheduler: Broadcasting large task binary with size 28.9 MiB\n",
      "25/11/23 21:50:50 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/11/23 21:50:54 WARN DAGScheduler: Broadcasting large task binary with size 37.9 MiB\n",
      "25/11/23 21:50:58 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado exitosamente en 57.17 segundos.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "print(\"--- ENTRENAMIENTO DEL MODELO (Random Forest) ---\")\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label_resultado\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=70,\n",
    "    maxDepth=15,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento con datos históricos...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Modelo entrenado exitosamente en {end_time - start_time:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7670366-7d82-41cc-965c-06a50b9885bf",
   "metadata": {},
   "source": [
    "## Generar predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc4dcd-af48-40c6-812c-7d0381a74426",
   "metadata": {},
   "source": [
    "* Transformar el set de prueba\n",
    "* Esto genera las columnas 'prediction' (0.0, 1.0, 2.0) y 'probability' (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea2aa65-724d-4592-9d13-5ca6857edf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO PREDICCIONES (Test Set) ---\n",
      "Predicciones realizadas.\n",
      "\n",
      "--- Ejemplo de Predicciones (Probabilidades) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:52:17 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 107:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+---------------+----------+------------------------------------------------------------+\n",
      "|fecha     |equipo_local|equipo_visitante|label_resultado|prediction|probability                                                 |\n",
      "+----------+------------+----------------+---------------+----------+------------------------------------------------------------+\n",
      "|2024-01-19|alaves      |cadiz           |0.0            |0.0       |[0.6026224239794182,0.20241173024367987,0.19496584577690196]|\n",
      "|2024-02-03|alaves      |barcelona       |1.0            |1.0       |[0.23191948145912994,0.5269736647539378,0.24110685378693225]|\n",
      "|2024-02-10|alaves      |villarreal      |2.0            |0.0       |[0.4200976565888217,0.29985853123584205,0.28004381217533625]|\n",
      "|2024-02-24|alaves      |mallorca        |2.0            |0.0       |[0.4547889988824172,0.28440987748672736,0.2608011236308555] |\n",
      "|2024-03-10|alaves      |vallecano       |0.0            |0.0       |[0.4310034207307532,0.24147023385516023,0.3275263454140865] |\n",
      "+----------+------------+----------------+---------------+----------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- GENERANDO PREDICCIONES (Test Set) ---\")\n",
    "\n",
    "predicciones_rf = rf_model.transform(test_data)\n",
    "\n",
    "print(\"Predicciones realizadas.\")\n",
    "\n",
    "print(\"\\n--- Ejemplo de Predicciones (Probabilidades) ---\")\n",
    "predicciones_rf.select(\n",
    "    \"fecha\", \"equipo_local\", \"equipo_visitante\",\n",
    "    \"label_resultado\", \"prediction\", \"probability\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e18e7-36bb-45ed-9ff9-2d1dcf78798c",
   "metadata": {},
   "source": [
    "Aplica el modelo a los datos de 2024–2025 y devuelve:\n",
    "\n",
    "* prediction → La clase predicha (0 = H, 1 = D, 2 = A)\n",
    "* probability → Vector de probabilidad [p_H, p_D, p_A]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a7b23-da57-4082-817c-d911b07424f7",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5882b5b-75c4-4217-b2d8-829863354ac8",
   "metadata": {},
   "source": [
    "### Accuracy (Exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310ca072-4ce4-414b-bf63-90b811ab3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÉTRICA 1: ACCURACY ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:52:40 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 108:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo: 0.5223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"=== MÉTRICA 1: ACCURACY ===\")\n",
    "\n",
    "eval_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_resultado\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = eval_accuracy.evaluate(predicciones_rf)\n",
    "print(f\"Accuracy del modelo: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee40b-18e6-414e-bfcf-32950b53d9b3",
   "metadata": {},
   "source": [
    "### F1-Score (Weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a65529f-b57d-42f5-8f4a-977f33c9d666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÉTRICA 2: F1-SCORE (Weighted) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:52:49 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 110:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score ponderado: 0.4664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=== MÉTRICA 2: F1-SCORE (Weighted) ===\")\n",
    "\n",
    "eval_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_resultado\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1_score = eval_f1.evaluate(predicciones_rf)\n",
    "print(f\"F1-Score ponderado: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac5eaa-a437-4e39-839b-d0c72da72844",
   "metadata": {},
   "source": [
    "### Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efadfdf9-85cc-4052-9daa-837067955db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÉTRICA 3: LOG LOSS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:52:54 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 112:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss del modelo: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "\n",
    "print(\"=== MÉTRICA 3: LOG LOSS ===\")\n",
    "\n",
    "# UDF para calcular logloss por fila\n",
    "def log_loss_udf(probabilities, label):\n",
    "    p = probabilities[int(label)]\n",
    "    p = max(min(p, 1 - 1e-15), 1e-15)  # evitar log(0)\n",
    "    return float(-math.log(p))\n",
    "\n",
    "logloss_udf = F.udf(log_loss_udf)\n",
    "\n",
    "# Cálculo de Log Loss fila a fila\n",
    "df_log = predicciones_rf.withColumn(\n",
    "    \"logloss_individual\",\n",
    "    logloss_udf(F.col(\"probability\"), F.col(\"label_resultado\"))\n",
    ")\n",
    "\n",
    "# Log Loss global\n",
    "logloss_global = df_log.agg(F.mean(\"logloss_individual\")).first()[0]\n",
    "\n",
    "print(f\"Log Loss del modelo: {logloss_global:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2a951-0fd7-405b-8a38-bdc4e08232f4",
   "metadata": {},
   "source": [
    "### Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fd0c82-b4c7-462e-a3c5-d19085fc7260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MATRIZ DE CONFUSIÓN (H-D-A) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 21:53:01 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "25/11/23 21:53:04 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 117:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----+\n",
      "|label_resultado|prediction|count|\n",
      "+---------------+----------+-----+\n",
      "|            0.0|       0.0| 1076|\n",
      "|            0.0|       1.0|  224|\n",
      "|            0.0|       2.0|   50|\n",
      "|            1.0|       0.0|  413|\n",
      "|            1.0|       1.0|  541|\n",
      "|            1.0|       2.0|   67|\n",
      "|            2.0|       0.0|  522|\n",
      "|            2.0|       1.0|  245|\n",
      "|            2.0|       2.0|   46|\n",
      "+---------------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=== MATRIZ DE CONFUSIÓN (H-D-A) ===\")\n",
    "\n",
    "predicciones_rf.groupBy(\n",
    "    \"label_resultado\", \"prediction\"\n",
    ").count().orderBy(\"label_resultado\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6077829-840f-4dbf-9f5f-40b4804aceb6",
   "metadata": {},
   "source": [
    "# Over2.5/Under2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49445efd-744d-4ccc-88fa-20e3f8969116",
   "metadata": {},
   "source": [
    "## GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b81b540-d975-45e6-9f55-2209f248c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CREANDO LABEL OVER/UNDER 2.5 ---\n",
      "+-----------+---------------+------------+\n",
      "|goles_local|goles_visitante|label_over25|\n",
      "+-----------+---------------+------------+\n",
      "|          1|              1|         0.0|\n",
      "|          3|              1|         1.0|\n",
      "|          1|              3|         1.0|\n",
      "|          1|              3|         1.0|\n",
      "|          0|              2|         0.0|\n",
      "+-----------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Etiqueta binaria creada correctamente.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "print(\"--- CREANDO LABEL OVER/UNDER 2.5 ---\")\n",
    "\n",
    "df_over = df_ml_ready.withColumn(\n",
    "    \"label_over25\",\n",
    "    when((col(\"goles_local\") + col(\"goles_visitante\")) >= 3, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "df_over.select(\"goles_local\", \"goles_visitante\", \"label_over25\").show(5)\n",
    "print(\"Etiqueta binaria creada correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccda75-8dc9-4caa-97cf-945acc7ec1e4",
   "metadata": {},
   "source": [
    "## División train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aadd4c04-d611-40d6-a96e-843c5aadaaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIVISIÓN TRAIN / TEST (POR FECHA) ---\n",
      "Train: 26164 filas\n",
      "Test:  3184 filas\n"
     ]
    }
   ],
   "source": [
    "print(\"--- DIVISIÓN TRAIN / TEST (POR FECHA) ---\")\n",
    "\n",
    "FECHA_CORTE = \"2024-01-01\"\n",
    "\n",
    "train_over = df_over.filter(col(\"fecha\") < FECHA_CORTE)\n",
    "test_over  = df_over.filter(col(\"fecha\") >= FECHA_CORTE)\n",
    "\n",
    "print(f\"Train: {train_over.count()} filas\")\n",
    "print(f\"Test:  {test_over.count()} filas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61beb1c2-aade-42bd-8631-5882760faafb",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DEL MODELO GBT (USANDO VECTOR features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d937bd4e-d85e-4329-a414-196b1405a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ENTRENANDO MODELO GBT (Over/Under 2.5) ---\n",
      "Modelo entrenado en 16.00 segundos.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "import time\n",
    "\n",
    "print(\"--- ENTRENANDO MODELO GBT (Over/Under 2.5) ---\")\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label_over25\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=40,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "inicio = time.time()\n",
    "gbt_model = gbt.fit(train_over)\n",
    "fin = time.time()\n",
    "\n",
    "print(f\"Modelo entrenado en {fin - inicio:.2f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687ab68-9003-4c3e-b7f5-3dfa0a8e1578",
   "metadata": {},
   "source": [
    "## Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "254bbe86-a9b5-4a5c-94bb-fd472e81e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO PREDICCIONES (TEST SET) ---\n",
      "+----------+------------+----------------+-----------+---------------+------------+----------------------------------------+----------+\n",
      "|fecha     |equipo_local|equipo_visitante|goles_local|goles_visitante|label_over25|probability                             |prediction|\n",
      "+----------+------------+----------------+-----------+---------------+------------+----------------------------------------+----------+\n",
      "|2024-01-19|alaves      |cadiz           |1          |0              |0.0         |[0.6223013473745512,0.37769865262544877]|0.0       |\n",
      "|2024-02-03|alaves      |barcelona       |1          |3              |1.0         |[0.3791256411405692,0.6208743588594308] |1.0       |\n",
      "|2024-02-10|alaves      |villarreal      |1          |1              |0.0         |[0.4709661521518405,0.5290338478481595] |1.0       |\n",
      "|2024-02-24|alaves      |mallorca        |1          |1              |0.0         |[0.5501286679154002,0.44987133208459984]|0.0       |\n",
      "|2024-03-10|alaves      |vallecano       |1          |0              |0.0         |[0.6464094768060057,0.3535905231939943] |0.0       |\n",
      "|2024-03-31|alaves      |sociedad        |0          |1              |0.0         |[0.6691230185752673,0.33087698142473265]|0.0       |\n",
      "|2024-04-21|alaves      |ath madrid      |2          |0              |0.0         |[0.47252800744233264,0.5274719925576674]|1.0       |\n",
      "|2024-04-27|alaves      |celta           |3          |0              |1.0         |[0.5799567079441127,0.4200432920558873] |0.0       |\n",
      "|2024-05-10|alaves      |girona          |2          |2              |1.0         |[0.3684338341410385,0.6315661658589615] |1.0       |\n",
      "|2024-05-18|alaves      |getafe          |1          |0              |0.0         |[0.639066572764767,0.36093342723523303] |0.0       |\n",
      "+----------+------------+----------------+-----------+---------------+------------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- GENERANDO PREDICCIONES (TEST SET) ---\")\n",
    "pred_over = gbt_model.transform(test_over)\n",
    "\n",
    "pred_over.select(\n",
    "    \"fecha\", \"equipo_local\", \"equipo_visitante\",\n",
    "    \"goles_local\", \"goles_visitante\",\n",
    "    \"label_over25\", \"probability\", \"prediction\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfdba30-f519-4be3-8123-806a7022f4e9",
   "metadata": {},
   "source": [
    "## EVALUACIÓN (AUC ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79396dd4-9653-44e4-9c65-ea70fdd6f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EVALUANDO MODELO (AUC ROC) ---\n",
      "AUC del modelo: 0.6008\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "print(\"--- EVALUANDO MODELO (AUC ROC) ---\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_over25\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(pred_over)\n",
    "print(f\"AUC del modelo: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcaff5-030f-4497-bd2d-0e2c2455b0f8",
   "metadata": {},
   "source": [
    "### Probab. del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a512aaa-5a13-423a-b322-9318d28bc2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+----------------+----------+------------+\n",
      "|fecha     |equipo_local|equipo_visitante|Prob_Modelo_Over|prediction|label_over25|\n",
      "+----------+------------+----------------+----------------+----------+------------+\n",
      "|2024-01-19|alaves      |cadiz           |0.37769866      |0.0       |0.0         |\n",
      "|2024-02-03|alaves      |barcelona       |0.62087435      |1.0       |1.0         |\n",
      "|2024-02-10|alaves      |villarreal      |0.52903384      |1.0       |0.0         |\n",
      "|2024-02-24|alaves      |mallorca        |0.44987133      |0.0       |0.0         |\n",
      "|2024-03-10|alaves      |vallecano       |0.35359052      |0.0       |0.0         |\n",
      "|2024-03-31|alaves      |sociedad        |0.33087698      |0.0       |0.0         |\n",
      "|2024-04-21|alaves      |ath madrid      |0.527472        |1.0       |0.0         |\n",
      "|2024-04-27|alaves      |celta           |0.4200433       |0.0       |1.0         |\n",
      "|2024-05-10|alaves      |girona          |0.63156617      |1.0       |1.0         |\n",
      "|2024-05-18|alaves      |getafe          |0.36093342      |0.0       |0.0         |\n",
      "+----------+------------+----------------+----------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "get_prob_over = udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "pred_probs = pred_over.withColumn(\"Prob_Modelo_Over\", get_prob_over(\"probability\"))\n",
    "\n",
    "pred_probs.select(\n",
    "    \"fecha\", \"equipo_local\", \"equipo_visitante\",\n",
    "    \"Prob_Modelo_Over\", \"prediction\", \"label_over25\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc688ac-c477-4e23-894b-fc74531cc0d4",
   "metadata": {},
   "source": [
    "## ANÁLISIS DE VALUE BETTING SI HAY CUOTA MAS 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b271a2d2-2e46-4466-95f8-4ef48ef99048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOP OPORTUNIDADES (Value Bets) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2450:>                                                       (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+-------------+----------------+-------------------+\n",
      "|     fecha|equipo_local|equipo_visitante|cuota_mas_2_5|Prob_Modelo_Over|              Valor|\n",
      "+----------+------------+----------------+-------------+----------------+-------------------+\n",
      "|2025-04-07|     bologna|          napoli|          2.5|      0.71690154|0.31690154075622556|\n",
      "|2024-09-18|       betis|          getafe|          3.0|      0.64360535|0.31027201811472577|\n",
      "|2025-05-10|    mallorca|      valladolid|          2.0|       0.7845036| 0.2845035791397095|\n",
      "|2024-09-25|   barcelona|          getafe|         1.57|       0.9148822|0.27793950791571553|\n",
      "|2025-10-26|        lyon|      strasbourg|         1.67|       0.8327308|0.23392843450614786|\n",
      "|2025-08-29|       elche|         levante|          2.3|       0.6662629|0.23148031597552088|\n",
      "|2024-05-11|    mallorca|      las palmas|         2.63|       0.5988938|0.21866568435734213|\n",
      "|2025-02-26|nottm forest|         arsenal|          2.2|      0.67155206| 0.2170066074891524|\n",
      "|2024-04-27|    paris sg|        le havre|         1.62|       0.8243624|0.20707844657662478|\n",
      "|2024-03-04|       inter|           genoa|         1.75|       0.7778698| 0.2064412491662162|\n",
      "+----------+------------+----------------+-------------+----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "if \"cuota_mas_2_5\" in df_over.columns:\n",
    "    final_value = pred_probs.withColumn(\n",
    "        \"Prob_Impl_Casa\", 1 / col(\"cuota_mas_2_5\")\n",
    "    ).withColumn(\n",
    "        \"Valor\", col(\"Prob_Modelo_Over\") - col(\"Prob_Impl_Casa\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- TOP OPORTUNIDADES (Value Bets) ---\")\n",
    "    final_value.filter(col(\"Valor\") > 0.05) \\\n",
    "        .select(\"fecha\", \"equipo_local\", \"equipo_visitante\",\n",
    "                \"cuota_mas_2_5\", \"Prob_Modelo_Over\", \"Valor\") \\\n",
    "        .orderBy(col(\"Valor\").desc()) \\\n",
    "        .show(10)\n",
    "else:\n",
    "    print(\"\\nNo existe la columna cuota_mas_2_5 en tu dataset, se omite Value Betting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c243c-1514-4bf6-bdb1-d90c7d786008",
   "metadata": {},
   "source": [
    "# Guardar modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c5d26-6786-47e3-8471-e905c7857136",
   "metadata": {},
   "source": [
    "## Modelo 1X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4e49fc8-1c02-40f4-a6b7-9cd5e9e5d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/24 02:30:24 WARN TaskSetManager: Stage 2455 contains a task of very large size (12829 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rf_model.save(\"hdfs:///user/johan/modelos/modelo_1x2_rf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dffe1e-bca4-461c-b362-edadea8097df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model.save(\"hdfs:///user/johan/modelos/modelo_over25_gbt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbcebe-8525-4258-99bc-88df7cdfbfb8",
   "metadata": {},
   "source": [
    "# Guardar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee3091-e116-4fe4-b8c9-484784e6e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_rf.write.mode(\"overwrite\").parquet(\"hdfs:///user/johan/salidas/predicciones_1x2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbab71-7b7b-4475-bad9-b504dddc738c",
   "metadata": {},
   "source": [
    "CSV NO soporta columnas complejas por eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dab4209c-ea98-4f7e-ab1a-83c623e0f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "predicciones_rf_conv = predicciones_rf.withColumn(\n",
    "    \"prob_array\", vector_to_array(\"probability\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d7bb1b1-1088-4cdd-8b65-2edf1e068223",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_rf_csv = predicciones_rf_conv.select(\n",
    "    \"fecha\",\n",
    "    \"equipo_local\",\n",
    "    \"equipo_visitante\",\n",
    "    \"label_resultado\",\n",
    "    \"prediction\",\n",
    "    col(\"prob_array\")[0].alias(\"prob_local\"),\n",
    "    col(\"prob_array\")[1].alias(\"prob_empate\"),\n",
    "    col(\"prob_array\")[2].alias(\"prob_visita\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f90e9dc6-2f82-4bd8-90e0-defca0831161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/24 02:59:07 WARN DAGScheduler: Broadcasting large task binary with size 25.5 MiB\n",
      "[Stage 2467:>                                                       (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+---------------+----------+-------------------+-------------------+-------------------+\n",
      "|     fecha|equipo_local|equipo_visitante|label_resultado|prediction|         prob_local|        prob_empate|        prob_visita|\n",
      "+----------+------------+----------------+---------------+----------+-------------------+-------------------+-------------------+\n",
      "|2024-01-19|      alaves|           cadiz|            0.0|       0.0| 0.6026224239794182|0.20241173024367987|0.19496584577690196|\n",
      "|2024-02-03|      alaves|       barcelona|            1.0|       1.0|0.23191948145912994| 0.5269736647539378|0.24110685378693225|\n",
      "|2024-02-10|      alaves|      villarreal|            2.0|       0.0| 0.4200976565888217|0.29985853123584205|0.28004381217533625|\n",
      "|2024-02-24|      alaves|        mallorca|            2.0|       0.0| 0.4547889988824172|0.28440987748672736| 0.2608011236308555|\n",
      "|2024-03-10|      alaves|       vallecano|            0.0|       0.0| 0.4310034207307532|0.24147023385516023| 0.3275263454140865|\n",
      "+----------+------------+----------------+---------------+----------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predicciones_rf_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6cafbdb-d7d4-4d44-8be7-2db9291634e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/24 02:56:15 WARN DAGScheduler: Broadcasting large task binary with size 25.7 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "predicciones_rf_csv.write.mode(\"overwrite\").csv(\"hdfs:///user/johan/salidas_csv/predicciones_1x2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39d231-908b-45c1-8d0d-ff9a75b4d792",
   "metadata": {},
   "source": [
    "## Modelo Over/Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a487463e-b06d-4acf-8364-f149dcdf59d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pred_probs.write.mode(\"overwrite\").parquet(\"hdfs:///user/johan/salidas/predicciones_over25/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e808d006-a723-474a-bdaf-8d83d506b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pred_probs_clean = pred_probs.withColumn(\n",
    "    \"prob_array\", vector_to_array(col(\"probability\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43839484-6a38-4e64-bc37-25bf33861428",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_over_csv = pred_probs_clean.select(\n",
    "    \"fecha\",\n",
    "    \"equipo_local\",\n",
    "    \"equipo_visitante\",\n",
    "    \"goles_local\",\n",
    "    \"goles_visitante\",\n",
    "    \"label_over25\",\n",
    "    \"prediction\",\n",
    "    col(\"prob_array\")[1].alias(\"prob_over25\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1a597bc-1b3a-4ef0-aa42-7471993ccdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+-----------+---------------+------------+----------+-------------------+\n",
      "|     fecha|equipo_local|equipo_visitante|goles_local|goles_visitante|label_over25|prediction|        prob_over25|\n",
      "+----------+------------+----------------+-----------+---------------+------------+----------+-------------------+\n",
      "|2024-01-19|      alaves|           cadiz|          1|              0|         0.0|       0.0|0.37769865262544877|\n",
      "|2024-02-03|      alaves|       barcelona|          1|              3|         1.0|       1.0| 0.6208743588594308|\n",
      "|2024-02-10|      alaves|      villarreal|          1|              1|         0.0|       1.0| 0.5290338478481595|\n",
      "|2024-02-24|      alaves|        mallorca|          1|              1|         0.0|       0.0|0.44987133208459984|\n",
      "|2024-03-10|      alaves|       vallecano|          1|              0|         0.0|       0.0| 0.3535905231939943|\n",
      "+----------+------------+----------------+-----------+---------------+------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_over_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1752467-c02e-4b2a-91db-f7f2a71b83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_over_csv.write.mode(\"overwrite\").csv(\n",
    "    \"hdfs:///user/johan/salidas_csv/predicciones_over25/\",\n",
    "    header=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f44241-e6cb-47a5-affe-7beae74ae414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
