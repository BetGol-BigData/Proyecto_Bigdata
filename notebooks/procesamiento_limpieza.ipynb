{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9950b1-1f4f-4504-af60-4a496fe087ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 01:15:37 WARN Utils: Your hostname, vbox resolves to a loopback address: 127.0.1.1; using 192.168.18.70 instead (on interface enp0s3)\n",
      "25/11/23 01:15:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 01:15:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/23 01:15:43 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "SparkSession iniciada correctamente\n",
      "Versión: 3.5.1\n",
      "Master: yarn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Java\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/zulu-8'\n",
    "\n",
    "# Python del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/debian1/BigData_UPAO/bigdata_env/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/debian1/BigData_UPAO/bigdata_env/bin/python3'\n",
    "\n",
    "# Archivos de configuración de Hadoop/YARN\n",
    "os.environ['HADOOP_CONF_DIR'] = '/opt/hadoop-3.3.6/etc/hadoop'\n",
    "os.environ['YARN_CONF_DIR'] = '/opt/hadoop-3.3.6/etc/hadoop'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"BetGol-ETL-ML\")\n",
    "        .master(\"yarn\")\n",
    "        .config(\"spark.driver.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"150\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"==============================================\")\n",
    "print(\"SparkSession iniciada correctamente\")\n",
    "print(\"Versión:\", spark.version)\n",
    "print(\"Master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5760beab-5f87-4bfb-a325-7534597e3904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Archivos detectados en HDFS/raw/: 85\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_10.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_11.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_12.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_13.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_14.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_15.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_16.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_17.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_2.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_3.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_4.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_5.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_6.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_7.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_8.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/D1_9.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_10.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_11.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_12.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_13.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_14.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_15.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_16.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_2.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_3.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_4.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_5.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_6.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_7.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_8.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/E0_9.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_10.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_11.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_12.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_13.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_14.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_15.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_16.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_17.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_2.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_3.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_4.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_5.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_6.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_7.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_8.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/F1_9.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_10.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_11.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_12.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_13.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_14.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_15.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_16.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_2.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_3.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_4.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_5.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_6.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_7.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_8.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/I1_9.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_1.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_10.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_11.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_12.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_13.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_14.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_15.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_16.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_2.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_3.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_4.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_5.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_6.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_7.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_8.csv\n",
      " - hdfs://localhost:9000/user/johan/data/raw/SP1_9.csv\n",
      "Columnas base definidas: 22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, coalesce, to_date, trim\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Carpeta RAW en HDFS\n",
    "ruta_hdfs = \"hdfs:///user/johan/data/raw/\"\n",
    "\n",
    "# Política de fechas antiguas\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Listar archivos desde HDFS\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "path = spark._jvm.org.apache.hadoop.fs.Path(ruta_hdfs)\n",
    "\n",
    "archivos_status = fs.listStatus(path)\n",
    "\n",
    "archivos_csv = [\n",
    "    f.getPath().toString()\n",
    "    for f in archivos_status\n",
    "    if f.getPath().getName().endswith(\".csv\")\n",
    "]\n",
    "\n",
    "archivos_csv.sort()\n",
    "\n",
    "cols_base = [\n",
    "    \"Div\", \"Date\", \"HomeTeam\", \"AwayTeam\",\n",
    "    \"FTHG\", \"FTAG\", \"FTR\",\n",
    "    \"HS\", \"AS\", \"HST\", \"AST\", \"HC\", \"AC\",\n",
    "    \"HF\", \"AF\", \"HY\", \"AY\", \"HR\", \"AR\",\n",
    "    \"B365H\", \"B365D\", \"B365A\"\n",
    "]\n",
    "\n",
    "print(\"======================================================\")\n",
    "print(f\"Archivos detectados en HDFS/raw/: {len(archivos_csv)}\")\n",
    "for a in archivos_csv:\n",
    "    print(\" -\", a)\n",
    "print(f\"Columnas base definidas: {len(cols_base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f6675c-dd0a-4694-8688-4689ce51c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Carga Inteligente desde HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completado. 85 DataFrames procesados.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando Carga Inteligente desde HDFS...\")\n",
    "\n",
    "lista_dfs_limpios = []\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    try:\n",
    "        # Leer archivo individualmente desde HDFS\n",
    "        df_temp = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(archivo)\n",
    "        )\n",
    "        \n",
    "        cols_actuales = df_temp.columns\n",
    "        exprs_select = []\n",
    "\n",
    "        # si falta una → NULL\n",
    "        for c in cols_base:\n",
    "            if c in cols_actuales:\n",
    "                exprs_select.append(col(f\"`{c}`\"))\n",
    "            else:\n",
    "                exprs_select.append(lit(None).alias(c))\n",
    "\n",
    "        # Over > 2.5\n",
    "        if \"B365>2.5\" in cols_actuales:\n",
    "            exprs_select.append(col(\"`B365>2.5`\").alias(\"B365>2.5\"))\n",
    "        elif \"BbAv>2.5\" in cols_actuales:\n",
    "            exprs_select.append(col(\"`BbAv>2.5`\").alias(\"B365>2.5\"))\n",
    "        else:\n",
    "            exprs_select.append(lit(None).alias(\"B365>2.5\"))\n",
    "\n",
    "        # Under < 2.5\n",
    "        if \"B365<2.5\" in cols_actuales:\n",
    "            exprs_select.append(col(\"`B365<2.5`\").alias(\"B365<2.5\"))\n",
    "        elif \"BbAv<2.5\" in cols_actuales:\n",
    "            exprs_select.append(col(\"`BbAv<2.5`\").alias(\"B365<2.5\"))\n",
    "        else:\n",
    "            exprs_select.append(lit(None).alias(\"B365<2.5\"))\n",
    "\n",
    "        # Construir DF limpio final\n",
    "        df_limpio = df_temp.select(exprs_select)\n",
    "        lista_dfs_limpios.append(df_limpio)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo {archivo}: {e}\")\n",
    "\n",
    "print(f\"Completado. {len(lista_dfs_limpios)} DataFrames procesados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dba6aa9-e73a-4f41-af3e-1a359600622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unificando datos desde 2010-2025...\n",
      "======================================================================\n",
      "CARGA EXITOSA DESDE HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Total de partidos cargados: 29513\n",
      "======================================================================\n",
      "\n",
      "--- Muestra de TODAS las columnas estandarizadas ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 01:47:44 WARN DAGScheduler: Broadcasting large task binary with size 1450.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-------------+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+-----+--------+--------+\n",
      "|Div|Date    |HomeTeam  |AwayTeam     |FTHG|FTAG|FTR|HS |AS |HST|AST|HC |AC |HF |AF |HY |AY |HR |AR |B365H|B365D|B365A|B365>2.5|B365<2.5|\n",
      "+---+--------+----------+-------------+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+-----+--------+--------+\n",
      "|D1 |07/08/09|Wolfsburg |Stuttgart    |2   |0   |H  |13 |14 |7  |4  |6  |3  |12 |12 |0  |0  |0  |0  |1.95 |3.5  |3.75 |1.59    |2.24    |\n",
      "|D1 |08/08/09|Dortmund  |FC Koln      |1   |0   |H  |24 |7  |11 |0  |16 |1  |8  |10 |0  |1  |0  |0  |1.62 |3.75 |5.5  |1.73    |2.02    |\n",
      "|D1 |08/08/09|Hertha    |Hannover     |1   |0   |H  |10 |15 |4  |3  |5  |3  |16 |20 |3  |2  |0  |0  |1.8  |3.5  |4.5  |1.8     |1.94    |\n",
      "|D1 |08/08/09|Hoffenheim|Bayern Munich|1   |1   |D  |9  |9  |1  |3  |3  |10 |10 |28 |0  |2  |0  |0  |4.2  |3.4  |1.91 |1.65    |2.14    |\n",
      "|D1 |08/08/09|Mainz     |Leverkusen   |2   |2   |D  |8  |13 |4  |7  |3  |5  |22 |28 |1  |2  |0  |0  |3.8  |3.4  |2.0  |1.67    |2.12    |\n",
      "+---+--------+----------+-------------+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+-----+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "print(\"Unificando datos desde 2010-2025...\")\n",
    "\n",
    "if len(lista_dfs_limpios) > 0:\n",
    "\n",
    "    # Unión vertical respetando nombres de columnas\n",
    "    df_f1_raw = reduce(DataFrame.unionByName, lista_dfs_limpios)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CARGA EXITOSA DESDE HDFS\")\n",
    "    print(f\"   - Total de partidos cargados: {df_f1_raw.count()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n--- Muestra de TODAS las columnas estandarizadas ---\")\n",
    "\n",
    "    # Se obtiene la lista de columnas en orden final\n",
    "    cols_finales = df_f1_raw.columns\n",
    "\n",
    "    # Mostrar primeras 5 filas protegiendo nombres con backticks\n",
    "    df_f1_raw.select([col(f\"`{c}`\") for c in cols_finales]).show(5, truncate=False)\n",
    "\n",
    "else:\n",
    "    print(\"No existen DataFrames limpios para unir.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be42c08-a174-4790-889f-b9bea83ae1b4",
   "metadata": {},
   "source": [
    "## Renombrado de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1817c7b-74ed-41d1-987d-3663863253da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Nuevo Esquema Estandarizado ----\n",
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- equipo_local: string (nullable = true)\n",
      " |-- equipo_visitante: string (nullable = true)\n",
      " |-- goles_local: integer (nullable = true)\n",
      " |-- goles_visitante: integer (nullable = true)\n",
      " |-- resultado_final: string (nullable = true)\n",
      " |-- tiros_totales_local: integer (nullable = true)\n",
      " |-- tiros_totales_visitante: integer (nullable = true)\n",
      " |-- tiros_a_puerta_local: integer (nullable = true)\n",
      " |-- tiros_a_puerta_visitante: integer (nullable = true)\n",
      " |-- corners_local: integer (nullable = true)\n",
      " |-- corners_visitante: integer (nullable = true)\n",
      " |-- faltas_local: integer (nullable = true)\n",
      " |-- faltas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_local: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_rojas_local: integer (nullable = true)\n",
      " |-- tarjetas_rojas_visitante: integer (nullable = true)\n",
      " |-- cuota_local: double (nullable = true)\n",
      " |-- cuota_empate: double (nullable = true)\n",
      " |-- cuota_visitante: double (nullable = true)\n",
      " |-- cuota_mas_2_5: double (nullable = true)\n",
      " |-- cuota_menos_2_5: double (nullable = true)\n",
      "\n",
      "\n",
      "--- Muestra de Datos ---\n",
      "+--------+--------+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "|division|   fecha|equipo_local|equipo_visitante|goles_local|goles_visitante|resultado_final|tiros_totales_local|tiros_totales_visitante|tiros_a_puerta_local|tiros_a_puerta_visitante|corners_local|corners_visitante|faltas_local|faltas_visitante|tarjetas_amarillas_local|tarjetas_amarillas_visitante|tarjetas_rojas_local|tarjetas_rojas_visitante|cuota_local|cuota_empate|cuota_visitante|cuota_mas_2_5|cuota_menos_2_5|\n",
      "+--------+--------+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "|      D1|07/08/09|   Wolfsburg|       Stuttgart|          2|              0|              H|                 13|                     14|                   7|                       4|            6|                3|          12|              12|                       0|                           0|                   0|                       0|       1.95|         3.5|           3.75|         1.59|           2.24|\n",
      "|      D1|08/08/09|    Dortmund|         FC Koln|          1|              0|              H|                 24|                      7|                  11|                       0|           16|                1|           8|              10|                       0|                           1|                   0|                       0|       1.62|        3.75|            5.5|         1.73|           2.02|\n",
      "|      D1|08/08/09|      Hertha|        Hannover|          1|              0|              H|                 10|                     15|                   4|                       3|            5|                3|          16|              20|                       3|                           2|                   0|                       0|        1.8|         3.5|            4.5|          1.8|           1.94|\n",
      "|      D1|08/08/09|  Hoffenheim|   Bayern Munich|          1|              1|              D|                  9|                      9|                   1|                       3|            3|               10|          10|              28|                       0|                           2|                   0|                       0|        4.2|         3.4|           1.91|         1.65|           2.14|\n",
      "|      D1|08/08/09|       Mainz|      Leverkusen|          2|              2|              D|                  8|                     13|                   4|                       7|            3|                5|          22|              28|                       1|                           2|                   0|                       0|        3.8|         3.4|            2.0|         1.67|           2.12|\n",
      "+--------+--------+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 01:56:32 WARN DAGScheduler: Broadcasting large task binary with size 1450.4 KiB\n"
     ]
    }
   ],
   "source": [
    "mapeo_columnas = {\n",
    "    # --- 1. Identificadores---\n",
    "    \"Div\": \"division\",\n",
    "    \"Date\": \"fecha\",\n",
    "    \"HomeTeam\": \"equipo_local\",\n",
    "    \"AwayTeam\": \"equipo_visitante\",\n",
    "\n",
    "    # --- Target / Resultados---\n",
    "    \"FTHG\": \"goles_local\",\n",
    "    \"FTAG\": \"goles_visitante\",\n",
    "    \"FTR\": \"resultado_final\",  # (H, D, A)\n",
    "\n",
    "    # --- Estadísticas de Juego ---\n",
    "    \"HS\": \"tiros_totales_local\",\n",
    "    \"AS\": \"tiros_totales_visitante\",\n",
    "    \"HST\": \"tiros_a_puerta_local\",\n",
    "    \"AST\": \"tiros_a_puerta_visitante\",\n",
    "    \"HC\": \"corners_local\",\n",
    "    \"AC\": \"corners_visitante\",\n",
    "    \"HY\": \"tarjetas_amarillas_local\",\n",
    "    \"AY\": \"tarjetas_amarillas_visitante\",\n",
    "    \"HR\": \"tarjetas_rojas_local\",\n",
    "    \"AR\": \"tarjetas_rojas_visitante\",\n",
    "\n",
    "    # Stats extra \n",
    "    \"HF\": \"faltas_local\",\n",
    "    \"AF\": \"faltas_visitante\",\n",
    "\n",
    "    # --- Cuotas de Negocio ---\n",
    "    \"B365H\": \"cuota_local\",\n",
    "    \"B365D\": \"cuota_empate\",\n",
    "    \"B365A\": \"cuota_visitante\",\n",
    "    \"B365>2.5\": \"cuota_mas_2_5\",\n",
    "    \"B365<2.5\": \"cuota_menos_2_5\"\n",
    "}\n",
    "\n",
    "df_renombre = df_f1_raw\n",
    "\n",
    "for col_antigua, col_nueva in mapeo_columnas.items():\n",
    "    df_renombre = df_renombre.withColumnRenamed(col_antigua, col_nueva)\n",
    "\n",
    "print(\"--- Nuevo Esquema Estandarizado ----\")\n",
    "df_renombre.printSchema()\n",
    "\n",
    "print(\"\\n--- Muestra de Datos ---\")\n",
    "df_renombre.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbe7c0-3015-4c79-b868-86f70ba5dc25",
   "metadata": {},
   "source": [
    "## Calidad de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93a097-a917-4d22-a093-4ef8e74b2b5e",
   "metadata": {},
   "source": [
    "### nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b946b85e-38dc-44d6-9614-fa66b8fba699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en F1 Renombrado: 29513\n",
      "\n",
      "--- REPORTE DE NULOS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:01:19 WARN DAGScheduler: Broadcasting large task binary with size 1274.9 KiB\n",
      "[Stage 178:===================================================>   (79 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " division                     | 8   \n",
      " fecha                        | 8   \n",
      " equipo_local                 | 8   \n",
      " equipo_visitante             | 8   \n",
      " goles_local                  | 8   \n",
      " goles_visitante              | 8   \n",
      " resultado_final              | 8   \n",
      " tiros_totales_local          | 11  \n",
      " tiros_totales_visitante      | 11  \n",
      " tiros_a_puerta_local         | 11  \n",
      " tiros_a_puerta_visitante     | 11  \n",
      " corners_local                | 11  \n",
      " corners_visitante            | 11  \n",
      " faltas_local                 | 13  \n",
      " faltas_visitante             | 13  \n",
      " tarjetas_amarillas_local     | 12  \n",
      " tarjetas_amarillas_visitante | 11  \n",
      " tarjetas_rojas_local         | 11  \n",
      " tarjetas_rojas_visitante     | 11  \n",
      " cuota_local                  | 17  \n",
      " cuota_empate                 | 17  \n",
      " cuota_visitante              | 17  \n",
      " cuota_mas_2_5                | 18  \n",
      " cuota_menos_2_5              | 18  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "print(f\"Total de filas en F1 Renombrado: {df_renombre.count()}\")\n",
    "print(\"\\n--- REPORTE DE NULOS ---\")\n",
    "\n",
    "df_renombre.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in df_renombre.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5eef7d-a480-4912-9a4c-1fb9a13dde3d",
   "metadata": {},
   "source": [
    "### Verificar nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2925f322-db28-4637-94cd-87d16ddc4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AUDITORÍA DE DUPLICADOS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partidos repetidos encontrados: 1\n",
      "\n",
      "Ejemplo de Partidos Repetidos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------------+------+\n",
      "|fecha|equipo_local|equipo_visitante|conteo|\n",
      "+-----+------------+----------------+------+\n",
      "| NULL|        NULL|            NULL|     8|\n",
      "+-----+------------+----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detalle para: None vs None el None\n",
      "+--------+-----+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "|division|fecha|equipo_local|equipo_visitante|goles_local|goles_visitante|resultado_final|tiros_totales_local|tiros_totales_visitante|tiros_a_puerta_local|tiros_a_puerta_visitante|corners_local|corners_visitante|faltas_local|faltas_visitante|tarjetas_amarillas_local|tarjetas_amarillas_visitante|tarjetas_rojas_local|tarjetas_rojas_visitante|cuota_local|cuota_empate|cuota_visitante|cuota_mas_2_5|cuota_menos_2_5|\n",
      "+--------+-----+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "+--------+-----+------------+----------------+-----------+---------------+---------------+-------------------+-----------------------+--------------------+------------------------+-------------+-----------------+------------+----------------+------------------------+----------------------------+--------------------+------------------------+-----------+------------+---------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "print(\"--- AUDITORÍA DE DUPLICADOS ---\")\n",
    "\n",
    "cols_clave = [\"fecha\", \"equipo_local\", \"equipo_visitante\"]\n",
    "\n",
    "df_duplicados = df_renombre \\\n",
    "    .groupBy(cols_clave) \\\n",
    "    .agg(count(\"*\").alias(\"conteo\")) \\\n",
    "    .filter(col(\"conteo\") > 1)\n",
    "\n",
    "# Contamos cuántos casos de duplicidad existen\n",
    "num_duplicados = df_duplicados.count()\n",
    "print(f\"Partidos repetidos encontrados: {num_duplicados}\")\n",
    "\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplo de Partidos Repetidos\")\n",
    "    df_duplicados.show(5)\n",
    "\n",
    "    fila = df_duplicados.first()\n",
    "    print(f\"Detalle para: {fila['equipo_local']} vs {fila['equipo_visitante']} el {fila['fecha']}\")\n",
    "\n",
    "    df_renombre.filter(\n",
    "        (col(\"fecha\") == fila[\"fecha\"]) &\n",
    "        (col(\"equipo_local\") == fila[\"equipo_local\"])\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd80f9-f4ae-4e84-885c-faa2637cefbd",
   "metadata": {},
   "source": [
    "### verificar si todos los duplicados son iguales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68c45318-3f3f-40c8-b597-48188ce8e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AUDITORÍA COMPARATIVA DE DUPLICADOS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:12:25 WARN DAGScheduler: Broadcasting large task binary with size 1673.6 KiB\n",
      "25/11/23 02:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1242.9 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados EXACTOS (Filas idénticas): 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:====================================================>  (81 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados LÓGICOS (Mismo partido): 1\n",
      "\n",
      "Los duplicados son copias exactas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "print(\"--- AUDITORÍA COMPARATIVA DE DUPLICADOS ---\")\n",
    "\n",
    "# filas iguales\n",
    "num_filas_total = df_renombre.count()\n",
    "num_filas_unicas = df_renombre.distinct().count()\n",
    "duplicados_exactos = num_filas_total - num_filas_unicas\n",
    "\n",
    "print(f\"Duplicados EXACTOS (Filas idénticas): {duplicados_exactos}\")\n",
    "\n",
    "#Mismo partido\n",
    "cols_clave = [\"fecha\", \"equipo_local\", \"equipo_visitante\"]\n",
    "duplicados_logicos = df_renombre.groupBy(cols_clave).count().filter(col(\"count\") > 1).count()\n",
    "\n",
    "print(f\"Duplicados LÓGICOS (Mismo partido): {duplicados_logicos}\")\n",
    "\n",
    "if duplicados_logicos > duplicados_exactos:\n",
    "    print(\"Hay filas que son el mismo partido pero tienen datos diferentes.\")\n",
    "    print(\"Usa la limpieza por Clave Lógica para no duplicar partidos en tu modelo.\")\n",
    "else:\n",
    "    print(\"\\nLos duplicados son copias exactas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae84310-6ece-438a-be41-7cfcbfe64a82",
   "metadata": {},
   "source": [
    "### Verificar Integridad de columnas categoricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1203675-8877-417c-a839-f49100375492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INTEGRIDAD CATEGÓRICA ---\n",
      "\n",
      "Ligas encontradas en 'division':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|division|\n",
      "+--------+\n",
      "|      D1|\n",
      "|      E0|\n",
      "|      F1|\n",
      "|      I1|\n",
      "|     SP1|\n",
      "|    NULL|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Resultados encontrados en 'resultado_final' (Solo debe haber H, D, A):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:=================================================>     (77 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|resultado_final|\n",
      "+---------------+\n",
      "|              A|\n",
      "|              H|\n",
      "|              D|\n",
      "|           NULL|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- INTEGRIDAD CATEGÓRICA ---\")\n",
    "\n",
    "print(\"\\nLigas encontradas en 'division':\")\n",
    "df_renombre.select(\"division\").distinct().show()\n",
    "\n",
    "print(\"\\nResultados encontrados en 'resultado_final' (Solo debe haber H, D, A):\")\n",
    "df_renombre.select(\"resultado_final\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229f26e-9ef8-41b4-9be4-365d3dc4d4da",
   "metadata": {},
   "source": [
    "### Verificamos columnas numericas mayores a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a3553c-9499-479b-ae73-dc64422e31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VALIDACIÓN NUMÉRICA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partidos con goles negativos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 217:================================================>      (75 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partidos con cuotas inválidas (<= 1.0): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- VALIDACIÓN NUMÉRICA ---\")\n",
    "\n",
    "# Goles Negativos\n",
    "errores_goles = df_renombre.filter(\n",
    "    (col(\"goles_local\").cast(\"int\") < 0) |\n",
    "    (col(\"goles_visitante\").cast(\"int\") < 0)\n",
    ")\n",
    "print(f\"Partidos con goles negativos: {errores_goles.count()}\")\n",
    "\n",
    "errores_cuotas = df_renombre.filter(\n",
    "    (col(\"cuota_local\").isNotNull()) &\n",
    "    (col(\"cuota_local\").cast(\"double\") <= 1.0) |\n",
    "\n",
    "    (col(\"cuota_empate\").isNotNull()) &\n",
    "    (col(\"cuota_empate\").cast(\"double\") <= 1.0) |\n",
    "\n",
    "    (col(\"cuota_visitante\").isNotNull()) &\n",
    "    (col(\"cuota_visitante\").cast(\"double\") <= 1.0)\n",
    ")\n",
    "print(f\"Partidos con cuotas inválidas (<= 1.0): {errores_cuotas.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd57726-8837-4dd8-a13d-865efd77fc53",
   "metadata": {},
   "source": [
    "### Verificar incoherencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9add0a-9da7-49ec-84a1-fb54c11567d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COHERENCIA DEPORTIVA (Goles vs Resultado) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:================================================>      (75 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partidos con resultados contradictorios: 0\n",
      "\n",
      "Todos los marcadores coinciden con su resultado (H/D/A).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- COHERENCIA DEPORTIVA (Goles vs Resultado) ---\")\n",
    "\n",
    "incoherencia_local = (col(\"goles_local\").cast(\"int\") > col(\"goles_visitante\").cast(\"int\")) & (col(\"resultado_final\") != \"H\")\n",
    "\n",
    "incoherencia_visita = (col(\"goles_local\").cast(\"int\") < col(\"goles_visitante\").cast(\"int\")) & (col(\"resultado_final\") != \"A\")\n",
    "\n",
    "incoherencia_empate = (col(\"goles_local\").cast(\"int\") == col(\"goles_visitante\").cast(\"int\")) & (col(\"resultado_final\") != \"D\")\n",
    "\n",
    "df_incoherencias = df_renombre.filter(incoherencia_local | incoherencia_visita | incoherencia_empate)\n",
    "\n",
    "conteo_errores = df_incoherencias.count()\n",
    "print(f\"Partidos con resultados contradictorios: {conteo_errores}\")\n",
    "\n",
    "if conteo_errores > 0:\n",
    "    print(\"\\nSe encontraron errores graves de lógica. Muestra:\")\n",
    "    df_incoherencias.select(\"fecha\", \"equipo_local\", \"equipo_visitante\",\n",
    "                            \"goles_local\", \"goles_visitante\", \"resultado_final\").show(5)\n",
    "else:\n",
    "    print(\"\\nTodos los marcadores coinciden con su resultado (H/D/A).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f8992-2c5c-488e-93a7-8b416cbf4aa8",
   "metadata": {},
   "source": [
    "# LIMPIEZA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7fa01-f810-4bec-ae85-52674a870c2b",
   "metadata": {},
   "source": [
    "# eliminar 8 filas vacias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5082bcb8-e85a-4612-b89b-61303dd18335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ELIMINACIÓN DE REGISTROS COMPLETAMENTE VACÍOS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:29:22 WARN DAGScheduler: Broadcasting large task binary with size 1189.7 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas completamente vacías eliminadas: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:29:26 WARN DAGScheduler: Broadcasting large task binary with size 1189.7 KiB\n",
      "[Stage 229:=================================================>     (77 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas restantes: 29505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- ELIMINACIÓN DE REGISTROS COMPLETAMENTE VACÍOS ---\")\n",
    "\n",
    "# how='all' elimina filas iguales\n",
    "df_f1_limpio = df_renombre.dropna(how='all')\n",
    "\n",
    "# Verificación\n",
    "filas_borradas = df_renombre.count() - df_f1_limpio.count()\n",
    "print(f\"Filas completamente vacías eliminadas: {filas_borradas}\")\n",
    "print(f\"Filas restantes: {df_f1_limpio.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b3911-3f2a-42f0-9005-3eee0aa72fc8",
   "metadata": {},
   "source": [
    "## Eliminar cuotas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025f568-98be-448c-ba5f-051bc685dd33",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- ELIMINACIÓN DE REGISTROS SIN CUOTAS ---\")\n",
    "\n",
    "col_criticas = [\"cuota_local\", \"cuota_empate\", \"cuota_visitante\", \"cuota_mas_2_5\", \"cuota_menos_2_5\"]\n",
    "\n",
    "df_limpieza_2 = df_f1_limpio.dropna(subset=col_criticas)\n",
    "\n",
    "# Verificación\n",
    "filas_borradas = df_f1_limpio.count() - df_limpieza_2.count()\n",
    "\n",
    "print(f\"Filas sin cuotas eliminadas: {filas_borradas}\")\n",
    "print(f\"Filas restantes y válidas: {df_limpieza_2.count()}\")\n",
    "\n",
    "print(\"\\nPaso de eliminación de cuotas completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289f03e-0790-4c26-8307-18fa42e17613",
   "metadata": {},
   "source": [
    "## imputar faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec1b82b-dd6d-49d6-b509-0526097b530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- IMPUTACIÓN DE ESTADÍSTICAS CON CERO ---\n",
      "Imputación con cero completada.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- IMPUTACIÓN DE ESTADÍSTICAS CON CERO ---\")\n",
    "\n",
    "col_imputar = [\n",
    "    \"tiros_totales_local\", \"tiros_totales_visitante\", \"tiros_a_puerta_local\",\n",
    "    \"tiros_a_puerta_visitante\", \"corners_local\", \"corners_visitante\",\n",
    "    \"faltas_local\", \"faltas_visitante\", \"tarjetas_amarillas_local\",\n",
    "    \"tarjetas_amarillas_visitante\", \"tarjetas_rojas_local\", \"tarjetas_rojas_visitante\"\n",
    "]\n",
    "\n",
    "df_imputado = df_limpieza_2.fillna(0, subset=col_imputar)\n",
    "\n",
    "print(\"Imputación con cero completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4adcfd-c6b1-41cf-8260-5330d28898b1",
   "metadata": {},
   "source": [
    "## Verificar si no hay nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aca7401-d4f7-4b21-b196-ea8de3ebc45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:35:09 WARN DAGScheduler: Broadcasting large task binary with size 1261.0 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas limpias: 29490\n",
      "\n",
      "--- REPORTE DE NULOS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:35:14 WARN DAGScheduler: Broadcasting large task binary with size 1847.1 KiB\n",
      "[Stage 244:===================================================>   (79 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " division                     | 0   \n",
      " fecha                        | 0   \n",
      " equipo_local                 | 0   \n",
      " equipo_visitante             | 0   \n",
      " goles_local                  | 0   \n",
      " goles_visitante              | 0   \n",
      " resultado_final              | 0   \n",
      " tiros_totales_local          | 0   \n",
      " tiros_totales_visitante      | 0   \n",
      " tiros_a_puerta_local         | 0   \n",
      " tiros_a_puerta_visitante     | 0   \n",
      " corners_local                | 0   \n",
      " corners_visitante            | 0   \n",
      " faltas_local                 | 0   \n",
      " faltas_visitante             | 0   \n",
      " tarjetas_amarillas_local     | 0   \n",
      " tarjetas_amarillas_visitante | 0   \n",
      " tarjetas_rojas_local         | 0   \n",
      " tarjetas_rojas_visitante     | 0   \n",
      " cuota_local                  | 0   \n",
      " cuota_empate                 | 0   \n",
      " cuota_visitante              | 0   \n",
      " cuota_mas_2_5                | 0   \n",
      " cuota_menos_2_5              | 0   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "print(f\"Total de filas limpias: {df_imputado.count()}\")\n",
    "print(\"\\n--- REPORTE DE NULOS ---\")\n",
    "\n",
    "df_imputado.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in df_imputado.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610de8f6-9b97-43e3-8d65-179125c1f0f4",
   "metadata": {},
   "source": [
    "## Verificar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d4c245c-045e-4cfb-88a6-e21d8e34c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUSCANDO PARTIDOS DUPLICADOS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:41:25 WARN DAGScheduler: Broadcasting large task binary with size 1665.3 KiB\n",
      "25/11/23 02:41:31 WARN DAGScheduler: Broadcasting large task binary with size 1255.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partidos duplicados encontrados: 0\n",
      "No hay duplicados según la clave lógica de partido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "print(\"=== BUSCANDO PARTIDOS DUPLICADOS ===\")\n",
    "\n",
    "cols_clave = [\"fecha\", \"equipo_local\", \"equipo_visitante\"]\n",
    "\n",
    "df_dup = (\n",
    "    df_imputado\n",
    "    .groupBy(cols_clave)\n",
    "    .agg(count(\"*\").alias(\"conteo\"))\n",
    "    .filter(col(\"conteo\") > 1)\n",
    ")\n",
    "\n",
    "num_dup = df_dup.count()\n",
    "\n",
    "print(f\"Partidos duplicados encontrados: {num_dup}\")\n",
    "\n",
    "if num_dup > 0:\n",
    "    print(\"\\n--- EJEMPLOS DE PARTIDOS DUPLICADOS ---\")\n",
    "    df_dup.show(10, truncate=False)\n",
    "else:\n",
    "    print(\"No hay duplicados según la clave lógica de partido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b0b84-1db2-4cac-bd77-b71294cb4d5d",
   "metadata": {},
   "source": [
    "# ESTANDARIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8873eb4c-cfec-4c4a-97c2-f01f3d703ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- equipo_local: string (nullable = true)\n",
      " |-- equipo_visitante: string (nullable = true)\n",
      " |-- goles_local: integer (nullable = true)\n",
      " |-- goles_visitante: integer (nullable = true)\n",
      " |-- resultado_final: string (nullable = true)\n",
      " |-- tiros_totales_local: integer (nullable = true)\n",
      " |-- tiros_totales_visitante: integer (nullable = true)\n",
      " |-- tiros_a_puerta_local: integer (nullable = true)\n",
      " |-- tiros_a_puerta_visitante: integer (nullable = true)\n",
      " |-- corners_local: integer (nullable = true)\n",
      " |-- corners_visitante: integer (nullable = true)\n",
      " |-- faltas_local: integer (nullable = true)\n",
      " |-- faltas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_local: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_rojas_local: integer (nullable = true)\n",
      " |-- tarjetas_rojas_visitante: integer (nullable = true)\n",
      " |-- cuota_local: double (nullable = true)\n",
      " |-- cuota_empate: double (nullable = true)\n",
      " |-- cuota_visitante: double (nullable = true)\n",
      " |-- cuota_mas_2_5: double (nullable = true)\n",
      " |-- cuota_menos_2_5: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imputado.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7e7aa-46a0-4620-bd4b-127fb8eab1e1",
   "metadata": {},
   "source": [
    "## Cambiar formato de los años que tienen YY a formato YYYY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb85ce-2384-4290-a7dd-32ce0126f62b",
   "metadata": {},
   "source": [
    "- Busca una barra \"/\" al final del texto, seguida de exactamente 2 dígitos.\n",
    "\n",
    "- Reemplaza \"/09\" por \"/2009\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b739768f-881f-413d-90ad-7d011e98f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CORRECCIÓN DEL AÑO ---\n",
      "Muestra de fechas corregidas (Texto):\n",
      "+----------+\n",
      "|     fecha|\n",
      "+----------+\n",
      "|07/08/2009|\n",
      "|08/08/2009|\n",
      "|08/08/2009|\n",
      "|08/08/2009|\n",
      "|08/08/2009|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:49:05 WARN DAGScheduler: Broadcasting large task binary with size 1458.3 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "print(\"--- CORRECCIÓN DEL AÑO ---\")\n",
    "\n",
    "df_fechas_arregladas = df_imputado.withColumn(\n",
    "    \"fecha\",\n",
    "    regexp_replace(col(\"fecha\"), r\"/(\\d{2})$\", r\"/20$1\")\n",
    ")\n",
    "\n",
    "print(\"Muestra de fechas corregidas (Texto):\")\n",
    "df_fechas_arregladas.select(\"fecha\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4893c945-4f3f-45bb-9422-b426b54ea7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:50:13 WARN DAGScheduler: Broadcasting large task binary with size 1422.0 KiB\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VERIFICAR SI TODAS SE CONVIRTIERON\n",
    "df_fechas_arregladas.select(\"fecha\").filter(col(\"fecha\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79280e02-12c7-42ad-85bc-822fdd6d8727",
   "metadata": {},
   "source": [
    "## Estandarizar fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa97f25-dce8-4b70-8c75-b8dd8a073cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONVERSIÓN A TIPO DATE ---\n",
      "Columna fecha convertida exitosamente.\n",
      "root\n",
      " |-- division: string (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- equipo_local: string (nullable = true)\n",
      " |-- equipo_visitante: string (nullable = true)\n",
      " |-- goles_local: integer (nullable = true)\n",
      " |-- goles_visitante: integer (nullable = true)\n",
      " |-- resultado_final: string (nullable = true)\n",
      " |-- tiros_totales_local: integer (nullable = true)\n",
      " |-- tiros_totales_visitante: integer (nullable = true)\n",
      " |-- tiros_a_puerta_local: integer (nullable = true)\n",
      " |-- tiros_a_puerta_visitante: integer (nullable = true)\n",
      " |-- corners_local: integer (nullable = true)\n",
      " |-- corners_visitante: integer (nullable = true)\n",
      " |-- faltas_local: integer (nullable = true)\n",
      " |-- faltas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_local: integer (nullable = true)\n",
      " |-- tarjetas_amarillas_visitante: integer (nullable = true)\n",
      " |-- tarjetas_rojas_local: integer (nullable = true)\n",
      " |-- tarjetas_rojas_visitante: integer (nullable = true)\n",
      " |-- cuota_local: double (nullable = true)\n",
      " |-- cuota_empate: double (nullable = true)\n",
      " |-- cuota_visitante: double (nullable = true)\n",
      " |-- cuota_mas_2_5: double (nullable = true)\n",
      " |-- cuota_menos_2_5: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:51:21 WARN DAGScheduler: Broadcasting large task binary with size 1534.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     fecha|\n",
      "+----------+\n",
      "|2009-08-07|\n",
      "|2009-08-08|\n",
      "|2009-08-08|\n",
      "|2009-08-08|\n",
      "|2009-08-08|\n",
      "|2009-08-08|\n",
      "|2009-08-08|\n",
      "|2009-08-09|\n",
      "|2009-08-09|\n",
      "|2009-08-15|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "print(\"--- CONVERSIÓN A TIPO DATE ---\")\n",
    "\n",
    "df_final = df_fechas_arregladas.withColumn(\n",
    "    \"fecha\",\n",
    "    to_date(col(\"fecha\"), \"dd/MM/yyyy\").cast(DateType())\n",
    ")\n",
    "\n",
    "print(\"Columna fecha convertida exitosamente.\")\n",
    "df_final.printSchema()\n",
    "df_final.select(\"fecha\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98a8b3-e192-45d3-9d6d-b4af8c79c2e4",
   "metadata": {},
   "source": [
    "## Estandarizar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76c9c7be-e8d6-4b28-827d-34269f643eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ESTANDARIZACIÓN TEXTO ---\n",
      "Nombres limpiados (sin tildes, minúsculas).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:54:04 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "[Stage 258:=====================================================> (83 + 2) / 85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|equipo_local |equipo_visitante|\n",
      "+-------------+----------------+\n",
      "|bayern munich|leverkusen      |\n",
      "|ein frankfurt|hamburg         |\n",
      "|hoffenheim   |mgladbach       |\n",
      "|werder bremen|stuttgart       |\n",
      "|werder bremen|nurnberg        |\n",
      "+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 02:54:10 WARN DAGScheduler: Broadcasting large task binary with size 1344.2 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, trim\n",
    "\n",
    "print(\"--- ESTANDARIZACIÓN TEXTO ---\")\n",
    "\n",
    "def quitar_tildes(columna):\n",
    "    c = col(columna)\n",
    "    c = regexp_replace(c, \"á\", \"a\")\n",
    "    c = regexp_replace(c, \"é\", \"e\")\n",
    "    c = regexp_replace(c, \"í\", \"i\")\n",
    "    c = regexp_replace(c, \"ó\", \"o\")\n",
    "    c = regexp_replace(c, \"ú\", \"u\")\n",
    "    c = regexp_replace(c, \"ñ\", \"n\")\n",
    "    c = regexp_replace(c, \"[^a-zA-Z0-9 ]\", \"\")\n",
    "    return trim(lower(c)) # Todo a minúsculas y sin espacios extra\n",
    "\n",
    "df_final = df_final \\\n",
    "    .withColumn(\"equipo_local\", quitar_tildes(\"equipo_local\")) \\\n",
    "    .withColumn(\"equipo_visitante\", quitar_tildes(\"equipo_visitante\"))\n",
    "\n",
    "print(\"Nombres limpiados (sin tildes, minúsculas).\")\n",
    "df_final.select(\"equipo_local\", \"equipo_visitante\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705eb7d2-e51d-4a03-9470-d88701b8d585",
   "metadata": {},
   "source": [
    "# GUARDAR LIMPIEZA EN PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2912518e-5d73-4d42-8886-050846d73cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 03:06:22 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_final.write.mode(\"overwrite\").parquet(\"hdfs:///user/johan/data/silver/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1113a-13e2-435d-b211-dba658bdabd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4faf6-1388-489b-9abb-8164f0ca4fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
